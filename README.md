# CS224N-Stanford-Winter-2019
The collection of ALL relevant materials about CS224N-Stanford/Winter 2019 course. THANKS TO THE PROFESSOR AND TAs!  
All the rights of the relevant materials belong to Standfor University.  

## Links
- Course page [CS224N](http://web.stanford.edu/class/cs224n/)
- Lecture videos 2019 [Youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
- Selected Solutions
  - https://github.com/Luvata/CS224N-2019/tree/master/Assignment
  - https://github.com/flypythoncom/cs224n_2019/tree/master/Assignment_1_intro_word_vectors
  - https://github.com/Observerspy/CS224n
  - https://looperxx.github.io/CS224n-2019-Assignment/
- https://zhuanlan.zhihu.com/p/78536912

## As1
- [x] lec1

### reading
- [ ] note: Word Vectors I: Introduction, SVD and Word2Ve
&nbsp;
- [x] Word2Vec Tutorial - The Skip-Gram Model
- [x] word2vec中的数学原理详解
- [x] Efficient Estimation of Word Representations in Vector Space (original word2vec paper)
- [x] Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper)

### practice
- [x] Computation on Arrays: Broadcasting
- [x] coding: Assignment1
- [x] Gensim


## As2
- [x] lec2
- [x] lec3
- [x] lec4

### reading
- [ ] note: Word Vectors II: GloVe, Evaluation and Trainin
- [ ] review-differential-calculus
- [ ] gradient-notes
- [ ] CS231n notes on network architectures
- [ ] CS231n notes on backprop
- [ ] backprop_old

### practice
- [x] python review
- [x] written: Assignment2
- [ ] coding: Assignment2


## As3
### reading
- [ ] note: Dependency Parsing 
- [ ] note: Language Models and Recurrent Neural Network
- [ ] a3

### practice
- [ ] written: Assignment3
- [ ] coding: Assignment3

## As4
### reading
- [ ] note:&emsp;Machine Translation, Sequence-to-sequence and Attention
- [ ] a4
- [ ] read:&emsp;Attention and Augmented Recurrent Neural Networks
- [ ] read:&emsp;Massive Exploration of Neural Machine Translation Architectures (practical advice for hyperparameter choices)

### practice
- [ ] written: Assignment4
- [ ] coding: Assignment4


### key point for a4
How to understand pack_padded_sequence and pad_packed_sequence?    
[(Chinese ed)](https://blog.csdn.net/lssc4205/article/details/79474735)    
[(English ed)](https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec)

## As5
### reading
- [ ] note:&emsp;Machine Translation, Sequence-to-sequence and Attention
- [ ] a4
- [ ] read:&emsp;Attention and Augmented Recurrent Neural Networks

### practice
- [ ] coding: Assignment5


## Final project
**reading**:

- [ ] final-project-practical-tips
- [ ] default-final-project-handout
- [ ] project-proposal-instructions
- [ ] Practical Methodology_Deep Learning book chapter
- [ ] Highway Networks
- [ ] Bidirectional Attention Flow for Machine Comprehension

**practice**:

- [ ] anotate codes
- [ ] train baseline