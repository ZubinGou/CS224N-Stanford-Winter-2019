# CS224N-Stanford-Winter-2019
The collection of ALL relevant materials about CS224N-Stanford/Winter 2019 course. THANKS TO THE PROFESSOR AND TAs!  
All the rights of the relevant materials belong to Standfor University.  

## Links
- Course page [CS224N](http://web.stanford.edu/class/cs224n/)
- Lecture videos 2019 [Youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
- Selected Solutions
  - https://github.com/Luvata/CS224N-2019/tree/master/Assignment
  - https://github.com/flypythoncom/cs224n_2019
  - https://looperxx.github.io/CS224n-2019-Assignment/
- https://zhuanlan.zhihu.com/p/78536912

## As1
- [x] lec1 Word Vectors

### reading
- [ ] note: Word Vectors I: Introduction, SVD and Word2Ve
&nbsp;
- [x] Word2Vec Tutorial - The Skip-Gram Model
- [x] word2vec中的数学原理详解
- [x] Efficient Estimation of Word Representations in Vector Space (original word2vec paper)
- [x] Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper)

### practice
- [x] Computation on Arrays: Broadcasting
- [x] coding: Assignment1
- [x] Gensim

## As2
- [x] lec2 Word Vectors 2 and Word Window Classification
- [x] lec3 Neural Networks
- [x] lec4 Backpropagation

### reading
- [ ] note: Word Vectors II: GloVe, Evaluation and Trainin
- [ ] review-differential-calculus
- [ ] gradient-notes
- [ ] CS231n notes on network architectures
- [ ] CS231n notes on backprop
- [ ] backprop_old

### practice
- [x] python review
- [x] written: Assignment2 Derivatives and implementation of word2vec algorithm
- [x] coding: Assignment2 Derivatives and implementation of word2vec algorithm


## As3
- [x] lec5 Dependency Parsing

### reading
- [ ] note: Dependency Parsing 
- [ ] note: Language Models and Recurrent Neural Network

### practice
- [x] PyTorch Tutorial
- [x] written: Assignment3 Dependency parsing and neural network foundations
- [x] coding: Assignment3 Dependency parsing and neural network foundations

## As4
- [x] lec6 Recurrent Neural Networks and Language Models
- [x] lec7 Vanishing Gradients, Fancy RNNs, Seq2Seq
- [x] lec8 Machine Translation, Attention, Subword Models
- [ ] Winter 2020 | Low Resource Machine Translation

### reading
- [x] lec6: The Unreasonable Effectiveness of Recurrent Neural Networks
- [x] lec7: Understanding LSTM Networks
- [x] lec8: Sequence to Sequence Learning with Neural Networks
- [x] lec8: Neural Machine Translation by Jointly Learning to Align and Translate
- [x] lec8: Attention and Augmented Recurrent Neural Networks
- [ ] lec8: Effective Approaches to Attention-based Neural Machine Translation
- [ ] lec8: Massive Exploration of Neural Machine Translation Architectures (practical advice for hyperparameter choices)
- [ ] lec8 note

### practice
- [x] written: Assignment4 Neural Machine Translation with sequence-to-sequence, attention, and subwords
- [x] coding: Assignment4 Neural Machine Translation with sequence-to-sequence, attention, and subwords

## As5
- [ ] lec9 Practical Tips for Projects
- [ ] lec10 Question Answering
- [ ] lec11 Convolutional Networks for NLP
- [ ] lec12 Subword Models
- [ ] lec13 Contextual Word Embeddings
- [ ] lec14 Transformers and Self-Attention
- [ ] lec15 Natural Language Generation
- [ ] Winter 2020 | BERT and Other Pre-trained Language Models

### reading
- [ ] note: Machine Translation, Sequence-to-sequence and Attention
- [ ] read: Attention and Augmented Recurrent Neural Networks

### practice
- [ ] written: Assignment5(2021) Self-supervised learning and fine-tuning with Transformers
- [ ] coding: Assignment5(2021) Self-supervised learning and fine-tuning with Transformers

## Final project
- [ ] lec17 Multitask Learning
- [ ] lec18 Constituency Parsing, TreeRNNs
- [ ] lec19 Bias in AI
- [ ] lec20 Future of NLP + Deep Learning

### reading
- [ ] final-project-practical-tips
- [ ] default-final-project-handout
- [ ] project-proposal-instructions
- [ ] Practical Methodology_Deep Learning book chapter
- [ ] Highway Networks
- [ ] Bidirectional Attention Flow for Machine Comprehension

### practice
- [ ] anotate codes
- [ ] train baseline
